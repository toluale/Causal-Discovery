# -*- coding: utf-8 -*-
"""Causal Learning on Audio Features, Real or Fake.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Y43dqrFzVOIsVUfK_du3Lsvoh2sPXbj
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import io
import requests

data = pd.read_csv('/content/drive/My Drive/ML/audio_feat.csv')

data.head()

"""#Data Preprocessing"""

data.drop('path', axis=1, inplace=True)

data.shape

#Created a dummy features for labels; dropped labels
dummies = pd.get_dummies(data['labels'])
data.head()

data2 = pd.concat([data, dummies], axis =1)

data2.drop('labels', axis=1, inplace=True)

data2.drop('real', axis=1, inplace=True)

data2 = data2.rename(columns={'fake':'label'})

#Feature label (fake:1; real:0)
data2

"""#Feature importance with perturbation rank"""

#Using perturbation rank for feature importance in MLP

def perturbation_rank (model, x, y, names, regression):
  errors = []

  for i in range(x.shape[1]):
    hold = np.array(x[:, i])
    np.random.shuffle(x[:, i])

    if regression:
      pred = model.predict(x)
      error = metrics.mean_squared_error(y, pred)
    else:
      pred = model.predict(x)
      error = metrics.log_loss(y, pred)

    errors.append(error)
    x[:, i] = hold

  max_error = np.max(errors)
  importance = [e/max_error for e in errors]

  data = {'name':names, 'error':errors, 'importance':importance}
  result = pd.DataFrame(data, columns=['name','error','importance'])
  result.sort_values(by=['importance'], ascending=[0], inplace=True)
  result.reset_index(inplace=True, drop=True)
  return result

from sklearn import metrics
from sklearn.preprocessing import MinMaxScaler
import requests
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation

from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from IPython.display import display, HTML
import sklearn

x= data2[['sd','median','mode','IQR','skew','kurt']].values
#dummies = pd.get_dummies(data['label'])
label=dummies.columns
y=dummies.values

#scaling the input features
scaler = MinMaxScaler()
x=scaler.fit_transform(x)

y.shape

x.shape

"""##Multi-Layer Perceptron"""

#split into train and test set
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.20, random_state=1001)

#multilayer perceptron
model = Sequential()
model.add(Dense(100, input_dim=x.shape[1], activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(y.shape[1], activation='sigmoid'))
model.compile(loss='categorical_crossentropy', optimizer='adam')
model.fit(x_train, y_train, verbose=0, epochs=1000)

from  sklearn.metrics import accuracy_score

#model accuracy
pred = model.predict(x_test)
predict_classes = np.argmax(pred, axis=1)
expected_classes = np.argmax(y_test, axis=1)
correct = accuracy_score(expected_classes, predict_classes)
print(f'Accuracy: {correct}')

"""##Feature Importance"""

# applying the perturbation rank function for feature ranking
names = list(data2.columns)
names.remove('label')
rank = perturbation_rank(model, x_test, y_test, names, False)
display(rank)

"""Applying Causal Discovery Algorithm"""

!pip install causal-learn

df = data2.values 
columns = data2.columns

"""#PC Algorithm

We combined the Fake and Real features into Label
"""

import networkx as nx
import matplotlib.pyplot as plt
from causallearn.search.ConstraintBased.PC import pc
from causallearn.utils.cit import kci
import statsmodels
import pydot

cg = pc(df, 0.05, kci)
# visualization using pydot
cg.draw_pydot_graph(labels=columns)
# or save the graph
from causallearn.utils.GraphUtils import GraphUtils
pyd = GraphUtils.to_pydot(cg.G)
pyd.write_png('df.png')
# visualization using networkx
#cg.to_nx_graph()
#cg.draw_nx_graph(skeleton)

"""Here we have the 2 target features, Fake $ Real as separate features"""

df2 = pd.concat([data, dummies], axis =1)
df2.drop('labels', axis=1, inplace=True)
df3 = df2.values 
#columns = data2.rename(columns={'real':'label'})
columns=df2.columns

cg2 = pc(df3, 0.05, kci)
# visualization using pydot
cg2.draw_pydot_graph(labels=columns)
# or save the graph
from causallearn.utils.GraphUtils import GraphUtils
pyd = GraphUtils.to_pydot(cg2.G)
pyd.write_png('df3.png')
# visualization using networkx
#cg.to_nx_graph()
#cg.draw_nx_graph(skeleton)